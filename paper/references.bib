@article{balki2019,
	author = {Indranil Balki and Afsaneh Amirabadi and Jacob Levman and Anne L. Martel and Ziga Emersic and Blaz Meden and Angel Garcia-Pedrero and Saul C. Ramirez and Dehan Kong and Alan R. Moody and Pascal N. Tyrrell},
	journal = {Canadian Association of Radiologists Journal},
    title = {Sample-Size Determination Methodologies for Machine Learning in Medical Imaging Research: A Systematic Review},
	number = {4},
	pages = {344-353},
	volume = {70},
	year = {2019}
}

@article{schraudolph2002,
author = {Schraudolph, Nicol N.},
title = {Fast curvature matrix-vector products for second-order gradient descent},
year = {2002},
issue_date = {July 2002},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {14},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/08997660260028683},
doi = {10.1162/08997660260028683},
abstract = {We propose a generic method for iteratively approximating various second-order gradient steps--Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient--in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD.},
journal = {Neural Comput.},
month = jul,
pages = {1723–1738},
numpages = {16}
}

@article{DBLP:journals/corr/SagunEGDB17,
  author       = {Levent Sagun and
                  Utku Evci and
                  V. Ugur G{\"{u}}ney and
                  Yann N. Dauphin and
                  L{\'{e}}on Bottou},
  title        = {Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1706.04454},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.04454},
  eprinttype    = {arXiv},
  eprint       = {1706.04454},
  timestamp    = {Mon, 22 Jul 2019 13:15:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SagunEGDB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1910-05929,
  author       = {Stanislav Fort and
                  Surya Ganguli},
  title        = {Emergent properties of the local geometry of neural loss landscapes},
  journal      = {CoRR},
  volume       = {abs/1910.05929},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.05929},
  eprinttype    = {arXiv},
  eprint       = {1910.05929},
  timestamp    = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-05929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{Matrixdifferential,
  abstract = {This book provides a self-contained and unified treatment of matrix differential calculus, aimed at econometricians and statisticians. It can be used as a textbook for senior undergraduate or graduate courses on the subject, and will also be a valuable source for professional econometricians and statisticians who want to learn and apply these important techniques. The authors base their approach on differentials rather than derivatives, and they show that the use of differentials is elegant, easy and considerably more useful in applications. No specialist knowledge of matrix algebra or calculus is required, since the basics of matrix algebra are covered in the first three chapters with a thorough treatment of multivariable calculus provided in Chapters Four to Seven. Exercises are included in each chapter, and many examples which illustrate applications of the theory are considered in detail.},
  added-at = {2014-03-16T13:03:09.000+0100},
  author = {Magnus, Jan R. and Neudecker, Heinz},
  biburl = {https://www.bibsonomy.org/bibtex/2c82317ecaf30079fa28c0fd5774e6780/ytyoun},
  edition = {Second},
  interhash = {76bd08d7316ac86cb1a7b88078010d57},
  intrahash = {c82317ecaf30079fa28c0fd5774e6780},
  isbn = {0471986321 9780471986324 047198633X 9780471986331},
  keywords = {calculus economics linear.algebra matrix textbook},
  publisher = {John Wiley},
  refid = {40467399},
  timestamp = {2016-05-31T14:11:33.000+0200},
  title = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
  year = 1999
}

@misc{
DissectingHessian,
title={Dissecting Hessian: Understanding Common Structure of Hessian in Neural Networks},
author={Yikai Wu and Xingyu Zhu and Chenwei Wu and Annie N. Wang and Rong Ge},
year={2021},
url={https://openreview.net/forum?id=0rNLjXgchOC}
}

@inproceedings{singh2023hessianperspectivenatureconvolutional,
author = {Singh, Sidak Pal and Hofmann, Thomas and Sch\"{o}lkopf, Bernhard},
title = {The Hessian perspective into the nature of convolutional neural networks},
year = {2023},
publisher = {JMLR.org},
abstract = {While Convolutional Neural Networks (CNNs) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their Hessian maps. The reason is that the loss Hessian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of CNNs get manifested in their structure and properties. We develop a framework relying on Toeplitz representation of CNNs, and then utilize it to reveal the Hessian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the Hessian rank and in practice also hold for more general settings. Overall, our work generalizes and further establishes the key insight that the Hessian rank grows as the square root of the number of parameters, even in CNNs.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1324},
numpages = {39},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}


@article{heschainrule,
  author       = {Maciej Skorski},
  title        = {Chain Rules for Hessian and Higher Derivatives Made Easy by Tensor
                  Calculus},
  journal      = {CoRR},
  volume       = {abs/1911.13292},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.13292},
  eprinttype    = {arXiv},
  eprint       = {1911.13292},
  timestamp    = {Wed, 08 Jan 2020 15:28:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-13292.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
toeplitz,
title={Toeplitz Neural Network for Sequence Modeling},
author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=IxmWsm4xrua}
}

@article{struct-rank-neural,
  author       = {Sidak Pal Singh and
                  Gregor Bachmann and
                  Thomas Hofmann},
  title        = {Analytic Insights into Structure and Rank of Neural Network Hessian
                  Maps},
  journal      = {CoRR},
  volume       = {abs/2106.16225},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.16225},
  eprinttype    = {arXiv},
  eprint       = {2106.16225},
  timestamp    = {Mon, 05 Jul 2021 15:15:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-16225.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{loss-landscape,
 author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Visualizing the Loss Landscape of Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{skip-conn-landscape,
author = {Wang, Lifu and Shen, Bo and Zhao, Ning and Zhang, Zhiyuan},
title = {Is the skip connection provable to reform the neural network loss landscape?},
year = {2021},
isbn = {9780999241165},
abstract = {The residual network is now one of the most effective structures in deep learning, which utilizes the skip connections to "guarantee" the performance will not get worse. However, the non-convexity of the neural network makes it unclear whether the skip connections do provably improve the learning ability since the nonlinearity may create many local minima. In some previous works [Freeman and Bruna, 2016], it is shown that despite the nonconvexity, the loss landscape of the two-layer ReLU network has good properties when the number m of hidden nodes is very large. In this paper, we follow this line to study the topology (sublevel sets) of the loss landscape of deep ReLU neural networks with a skip connection and theoretically prove that the skip connection network inherits the good properties of the two-layer network and skip connections can help to control the connectedness of the sub-level sets, such that any local minima worse than the global minima of some two-layer ReLU network will be very "shallow". The "depth" of these local minima are at most O(m(η-1)/n), where n is the input dimension, η < 1. This provides a theoretical explanation for the effectiveness of the skip connection in deep learning.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {387},
numpages = {7},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}


@inproceedings{Bosman_2023, 
   series={GECCO ’23 Companion},
   title={Empirical Loss Landscape Analysis of Neural Network Activation Functions},
   volume={33},
   url={http://dx.doi.org/10.1145/3583133.3596321},
   DOI={10.1145/3583133.3596321},
   booktitle={Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
   publisher={ACM},
   author={Bosman, Anna Sergeevna and Engelbrecht, Andries and Helbig, Marde},
   year={2023}
}

@article{BOSMAN2020113,
title = {Visualising basins of attraction for the cross-entropy and the squared error neural network loss functions}, 
author = {Anna Sergeevna Bosman, Andries Engelbrecht, Mardé Helbig},
journal = {Neurocomputing},
volume = {400},
pages = {113-136},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.113},
url ={https://www.sciencedirect.com/science/article/pii/S0925231220303593},
}

@inproceedings{
neyshabur2018the,
title={The role of over-parametrization in generalization of neural networks},
author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BygfghAcYX},
}

@inproceedings{improved_analysis_trianing,
 author = {Zou, Difan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An Improved Analysis of Training Over-parameterized Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{singh2024landscapinglinearmodeconnectivity,
      title={Landscaping Linear Mode Connectivity}, 
      author={Sidak Pal Singh and Linara Adilova and Michael Kamp and Asja Fischer and Bernhard Schölkopf and Thomas Hofmann},
      year={2024},
      eprint={2406.16300},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.16300}, 
}

@misc{categ_plots,
      title={Visualizing, Rethinking, and Mining the Loss Landscape of Deep Neural Networks}, 
      author={Xin-Chun Li and Lan Li and De-Chuan Zhan},
      year={2024},
      eprint={2405.12493},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.12493}, 
}

@misc{papyan2019spectrumdeepnethessiansscale,
      title={The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size}, 
      author={Vardan Papyan},
      year={2019},
      eprint={1811.07062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.07062}, 
}

@inproceedings{liao2021hessianeigenspectrarealisticnonlinear,
author = {Liao, Zhenyu and Mahoney, Michael W.},
title = {Hessian eigenspectra of more realistic nonlinear models},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Given an optimization problem, the Hessian matrix and its eigenspectrum can be used in many ways, ranging from designing more efficient second-order algorithms to performing model analysis and regression diagnostics. When nonlinear models and non-convex problems are considered, strong simplifying assumptions are often made to make Hessian spectral analysis more tractable. This leads to the question of how relevant the conclusions of such analyses are for realistic nonlinear models. In this paper, we exploit tools from random matrix theory to make a precise characterization of the Hessian eigenspectra for a broad family of nonlinear models that extends the classical generalized linear models, without relying on strong simplifying assumptions used previously. We show that, depending on the data properties, the nonlinear response model, and the loss function, the Hessian can have qualitatively different spectral behaviors: of bounded or unbounded support, with single- or multi-bulk, and with isolated eigenvalues on the left- or right-hand side of the main eigenvalue bulk. By focusing on such a simple but nontrivial model, our analysis takes a step forward to unveil the theoretical origin of many visually striking features observed in more realistic machine learning models.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1538},
numpages = {14},
series = {NIPS '21}
}

@misc{sagun2017eigenvalueshessiandeeplearning,
      title={Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond}, 
      author={Levent Sagun and Leon Bottou and Yann LeCun},
      year={2017},
      eprint={1611.07476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.07476}, 
}

@misc{kiselev2024unravelinghessiankeysmooth,
      title={Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes}, 
      author={Nikita Kiselev and Andrey Grabovoy},
      year={2024},
      eprint={2409.11995},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.11995}, 
}

@misc{singh2021analyticinsightsstructurerank,
      title={Estimating the Jacobian matrix of an unknown multivariate function from sample values by means of a neural network}, 
      author={Frédéric Latrémolière and Sadananda Narayanappa and Petr Vojtěchovský},
      year={2022},
      eprint={2204.00523},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2204.00523}, 
}

@article{kohn2022geometrylinearconvolutionalnetworks,
author = {Kohn, Kathl\'{e}n and Merkh, Thomas and Mont\'{u}far, Guido and Trager, Matthew},
title = {Geometry of Linear Convolutional Networks},
journal = {SIAM Journal on Applied Algebra and Geometry},
volume = {6},
number = {3},
pages = {368-406},
year = {2022},
doi = {10.1137/21M1441183},
URL = {https://doi.org/10.1137/21M1441183},
eprint = { 
        https://doi.org/10.1137/21M1441183
},
abstract = { We study the family of functions that are represented by a linear convolutional network (LCN). These functions form a semi-algebraic subset of the set of linear maps from input space to output space. In contrast, the families of functions represented by fully connected linear networks form algebraic sets. We observe that the functions represented by LCNs can be identified with polynomials that admit certain factorizations, and we use this perspective to describe the impact of the network's architecture on the geometry of the resulting function space. We further study the optimization of an objective function over an LCN, analyzing critical points in function space and in parameter space and describing dynamical invariants for gradient descent. Overall, our theory predicts that the optimized parameters of an LCN will often correspond to repeated filters across layers, or filters that can be decomposed as repeated filters. We also conduct numerical and symbolic experiments that illustrate our results and present an in-depth analysis of the landscape for small architectures. }
}

@misc{qin2023toeplitzneuralnetworksequence,
      title={Toeplitz Neural Network for Sequence Modeling}, 
      author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
      year={2023},
      eprint={2305.04749},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.04749}, 
}

@InProceedings{investigation_neural_net_eigen,
  title = 	 {An Investigation into Neural Net Optimization via Hessian Eigenvalue Density},
  author =       {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2232--2241},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ghorbani19b/ghorbani19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ghorbani19b.html},
  abstract = 	 {To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to partially explain these phenomena. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications.}
}


@article{full_spec_hess,
  author       = {Vardan Papyan},
  title        = {The Full Spectrum of Deep Net Hessians At Scale: Dynamics with Sample
                  Size},
  journal      = {CoRR},
  volume       = {abs/1811.07062},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.07062},
  eprinttype    = {arXiv},
  eprint       = {1811.07062},
  timestamp    = {Sun, 25 Nov 2018 18:57:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-07062.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{rethinking_effective,
  author       = {Wesley J. Maddox and
                  Gregory W. Benton and
                  Andrew Gordon Wilson},
  title        = {Rethinking Parameter Counting in Deep Models: Effective Dimensionality
                  Revisited},
  journal      = {CoRR},
  volume       = {abs/2003.02139},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.02139},
  eprinttype    = {arXiv},
  eprint       = {2003.02139},
  timestamp    = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-02139.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{azadbakht2022drasticallyreducingnumbertrainable,
      title={Drastically Reducing the Number of Trainable Parameters in Deep CNNs by Inter-layer Kernel-sharing}, 
      author={Alireza Azadbakht and Saeed Reza Kheradpisheh and Ismail Khalfaoui-Hassani and Timothée Masquelier},
      year={2022},
      eprint={2210.14151},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2210.14151}, 
}


@article{wu2019prodsumnetreducingmodelparameters,
author = {Kroshchanka, A. A. and Golovko, V. A. and Chodyka, M.},
title = {Method for Reducing Neural-Network Models of Computer Vision},
year = {2022},
issue_date = {Jun 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2},
issn = {1054-6618},
url = {https://doi.org/10.1134/S1054661822020146},
doi = {10.1134/S1054661822020146},
journal = {Pattern Recognit. Image Anal.},
month = jun,
pages = {294–300},
numpages = {7},
keywords = {computer vision, pretraining of deep neural networks, reduction of neural network parameters, deep neural networks}
}

@INPROCEEDINGS{kahatapitiya2020exploitingredundancyconvolutionalfilters,
  author={Kahatapitiya, Kumara and Rodrigo, Ranga},
  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Exploiting the Redundancy in Convolutional Filters for Parameter Reduction}, 
  year={2021},
  volume={},
  number={},
  pages={1409-1419},
  keywords={Computer vision;Correlation;Computational modeling;Conferences;Redundancy;Memory management;Network architecture},
  doi={10.1109/WACV48630.2021.00145}}

@InProceedings{papyan2019measurementsthreelevelhierarchicalstructure,
  title = 	 {Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians},
  author =       {Papyan, Vardan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5012--5021},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/papyan19a/papyan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/papyan19a.html},
  abstract = 	 {We expose a structure in deep classifying neural networks in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain "averaging" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes.}
}


@article{papyan2020tracesclasscrossclassstructurepervade,
  author  = {Vardan Papyan},
  title   = {Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {252},
  pages   = {1--64},
  url     = {http://jmlr.org/papers/v21/20-933.html}
}

@inproceedings{
singh2022phenomenologydoubledescentfinitewidth,
title={Phenomenology of Double Descent in Finite-Width Neural Networks},
author={Sidak Pal Singh and Aurelien Lucchi and Thomas Hofmann and Bernhard Sch{\"o}lkopf},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=lTqGXfn9Tv}
}

@misc{skorski2019chainruleshessianhigher,
      title={Chain Rules for Hessian and Higher Derivatives Made Easy by Tensor Calculus}, 
      author={Maciej Skorski},
      year={2019},
      eprint={1911.13292},
      archivePrefix={arXiv},
      primaryClass={cs.SC},
      url={https://arxiv.org/abs/1911.13292}, 
}


@unknown{toep_2dconv,
author = {Gnacik, Michal and Łapa, Krystian},
year = {2022},
month = {10},
pages = {},
title = {Using Toeplitz Matrices to obtain 2D convolution},
doi = {10.21203/rs.3.rs-2195496/v1}
}


@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}

@misc{simonyan2015deepconvolutionalnetworkslargescale,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556}, 
}

@misc{huang2018denselyconnectedconvolutionalnetworks,
      title={Densely Connected Convolutional Networks}, 
      author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
      year={2018},
      eprint={1608.06993},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1608.06993}, 
}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{lee2024visualizinglosslandscapeselfsupervised,
      title={Visualizing the loss landscape of Self-supervised Vision Transformer}, 
      author={Youngwan Lee and Jeffrey Ryan Willette and Jonghee Kim and Sung Ju Hwang},
      year={2024},
      eprint={2405.18042},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.18042}, 
}

@misc{bain2021visualizinglosslandscapewinning,
      title={Visualizing the Loss Landscape of Winning Lottery Tickets}, 
      author={Robert Bain},
      year={2021},
      eprint={2112.08538},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.08538}, 
}



@inproceedings{
chen2022visiontransformersoutperformresnets,
title={When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations},
author={Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=LtKcMgGOeLt}
}

@misc{elhamod2023neurovisualizerautoencoderbasedlosslandscape,
      title={Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method}, 
      author={Mohannad Elhamod and Anuj Karpatne},
      year={2023},
      eprint={2309.14601},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.14601}, 
}

@article{Exp_explor_loss_surface,
author = {Yuan, Qunyong and Xiao, Nanfeng},
title = {Experimental exploration on loss surface of deep neural network},
journal = {International Journal of Imaging Systems and Technology},
volume = {30},
number = {4},
pages = {860-873},
keywords = {loss surface of deep neural network, Hessian matrix deep neural network, the trajectories of the various adaptive optimizations, curvature of the loss surface, ensemble learning},
doi = {https://doi.org/10.1002/ima.22434},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ima.22434},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ima.22434},
year = {2020}
}

@misc{im2017empiricalanalysisoptimizationdeep,
      title={An empirical analysis of the optimization of deep network loss surfaces}, 
      author={Daniel Jiwoong Im and Michael Tao and Kristin Branson},
      year={2017},
      eprint={1612.04010},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1612.04010}, 
}


@article{MNIST,
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]},
  author={Li Deng},
  journal={IEEE Signal Processing Magazine},
  year={2012},
  volume={29},
  pages={141-142},
  url={https://api.semanticscholar.org/CorpusID:5280072}
}i

@misc{xiao2017fashionmnistnovelimagedataset,
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images
of 70,000 fashion products from 10 categories, with 7,000 images per category.
The training set has 60,000 images and the test set has 10,000 images.
Fashion-MNIST is intended to serve as a direct drop-in replacement for the
original MNIST dataset for benchmarking machine learning algorithms, as it
shares the same image size, data format and the structure of training and
testing splits. The dataset is freely available at
https://github.com/zalandoresearch/fashion-mnist},
  added-at = {2021-10-12T06:50:19.000+0200},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  biburl = {https://www.bibsonomy.org/bibtex/2de51af2f6c7d8b0f4cd84a428bb17967/andolab},
  description = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  interhash = {0c81f9a6170118f14703b6796101ce40},
  intrahash = {de51af2f6c7d8b0f4cd84a428bb17967},
  keywords = {Fashion-MNIST Image_Classification_Benchmark},
  note = {cite arxiv:1708.07747Comment: Dataset is freely available at  https://github.com/zalandoresearch/fashion-mnist Benchmark is available at  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/},
  timestamp = {2023-01-31T20:34:07.000+0100},
  title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
  Algorithms},
  url = {http://arxiv.org/abs/1708.07747},
  year = 2017
}


@inproceedings{CIFAR10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}

@article{sample-size-det,
author = {Grabovoy, Andrey and Gadaev, Tamaz and Motrenko, A. and Strijov, Vadim},
year = {2022},
month = {12},
pages = {2453-2462},
title = {Numerical Methods of Sufficient Sample Size Estimation for Generalised Linear Models},
volume = {43},
journal = {Lobachevskii Journal of Mathematics},
doi = {10.1134/S1995080222120125}
}

@article{relevance, 
title = {ESTIMATION OF THE RELEVANCE OF THE NEURAL NETWORK PARAMETERs},
author = {Grabovoy,Andrey and Bakhteev,Oleg and Strijov,V},
ISSN={1992-2264}, 
url={http://dx.doi.org/10.14357/19922264190209}, 
DOI={10.14357/19922264190209}, 
journal={Informatics and Applications}, 
publisher={Federal Research Center &quot;Computer Science and Control&quot; of the Russian Academy of Sciences}, 
year={2019}, 
month=jun 
}


@misc{shinshilla,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@article{ordering, 
  author = {Grabovoy,Andrey and Bakhteev, O and Strijov,V},
  title = {ORDERING THE SET OF NEURAL NETWORK PARAMETERS},
  ISSN = {1992-2264},
  url = {http://dx.doi.org/10.14357/19922264200208},
  DOI = {10.14357/19922264200208},
  journal = {Informatics and Applications},
  publisher = {Federal Research Center &quot;Computer Science and Control&quot; of the Russian Academy of Sciences},
  year = {2020},
  month = jun 
}

@misc{
hayou2024a,
title={A Theoretical Study of the Jacobian Matrix in Deep Neural Networks},
author={Soufiane Hayou and Benjamin Dadoun and Pierre Youssef and Hanan Salam and Mohamed El Amine Seddik},
year={2024},
url={https://openreview.net/forum?id=pvhyBB86Bt}
}